import processing.sound.*;

/**
 * AudioCapture shows how you can capture streaming audio from live input or from a file.
 * Using the Sound library in Processing, the control panel for Sound in MacOS, and the
 * BlackHole audio routing tool (https://existential.audio/blackhole/), you can obtain
 * a wide range of inputs and outputs, depending on your system software and hardware. 
 * YMMV, using Processing's Sound library. I'm using Minim. Some suggestions follow. 
 * 
 * For example, in MacOS: 
 *   Ignore Sound.inputDevice() and Sound.outputDevice(), use the System Settings instead.
 *   Set Output to BlackHole 16ch
 *   Set Input to your external audio hardware, for an external mic: mine is Volt2
 *   
 * Then in Max Audio Status control panel:
 *   Set Input Device to BlackHole 16ch
 *   Set Output Device to your external audio hardware, e.g. Volt2
 *   Create a patcher that gets signals from an adc~, route the adc~ to some sort of effects 
 *   (I just used a tapin~/tapout~ pair), pass the outputs to a gain~ and then to a dac~. 
 * 
 * I've provided a sample Max patcher, simpleAudioIO.maxpat, in the "data" folder for this sketch.
 * 
 * You can also configure this sketch to capture audio signals generated by another application. 
 * 
 * For example, using Max, open a patcher that generates and outputs audio, such as 
 * 05yFMSynthesis.maxpat in the MSP tutorial. 
 *   In MacOS Audio MIDI Setup tool, configure a Multi-Output Device to use BlackHole, your built-in
 *   speakers, and any other hardware output. 
 *   In the System Settings Sound panel, configure Input to BlackHole and Output to Multi-Channel Device.
 *   In Max's Audio Status control panel, set Input to BlackHole and Output to  Multi-Channel Device.
 *   
 * Run the AudioCapture application and start listening (spacebar turns listening on and off). 
 * Play a sound in Max. It should appear as pixel patterns in the AudioCapture window. 
 * 
 * 
 * In the Eclipse IDE in MacOS, inputs do not seem to function. Streaming from a file
 * works as expected, using Minim. Outputs should be set with the System Sound control panel. 
 * 
 */

import java.util.Arrays;

import net.paulhertz.pixelaudio.*;
import processing.core.*;
import ddf.minim.*;
import ddf.minim.ugens.*;


/** PixelAudio library */
public PixelAudio pixelaudio;
/** A PixelMapGen to generate the signal path */
HilbertGen hGen;
/** A PixelAudioMapper to negotiate between image and audio */
PixelAudioMapper mapper;
/** the color channel we will draw to */
PixelAudioMapper.ChannelNames chan;

/** image for display */
PImage mapImage;
/** the number of pixels in the image, the number of samples in the audio signal, etc. */
int mapSize;
/** an array of colors through the RGB spectrum, handy for showing the shape of the signal path */
int[] colors;

/** Minim audio library */
Minim minim;
AudioInput audioIn;
AudioOutput audioOut;
/** A buffer for audio that we stream */
MultiChannelBuffer audioBuffer;
/** A Minim audio sampler to construct a sampling instrument */
Sampler audioSampler;
/** An audio recorder for capturing input from the microphone */
AudioRecorder recorder;
/** Our homemade audio sampling instrument */
WFSamplerInstrument instrument;
/** A source for streaming audio from a file */
AudioPlayer anthem;
/** The class that captures audio for us */
StreamCapture streamCap;
/** audio signal as an array of floats */
float[] audioSignal;
/** audio signal transcoded to RGB data */
int[] rgbSignal;
int audioLength;
/** audio sampling rate */
int sampleRate = 44100;
/** duration of a sample played by the WFInstrument, in seconds */
float sampleLength = 1.0f;
/** ADSR and parameters */
ADSR adsr;
float maxAmplitude = 0.9f;
float attackTime = 0.2f;
float decayTime = 0.125f;
float sustainLevel = 0.5f;
float releaseTime = 0.2f;
/** audio variables */
boolean listening = false;
boolean listenLive = true;


/**
 *  For the HilbertGen we require width == height == a power of 2. 
 */
public void settings() {
  size(512, 512);
}

public void setup() {
  pixelaudio = new PixelAudio(this);
  initMapper();
  mapSize = mapper.getSize();
  minim = new Minim(this);
  initAudio();
  Sound.list();
  // set the devices you're using, depending on values from Sound.list()
  // works with Processing Sound library, but we're using Minim. 
  // In MacOS with Minim, use the System Settings control panel and AudioMIDI Setup.
  // You should be able to do something similar in Windows. 
  //Sound.inputDevice(6);
  //Sound.outputDevice(6);
  showHelp();
}

public int[] getColors() {
  int[] colorWheel = new int[mapper.getSize()];
  pushStyle();
  colorMode(HSB, colorWheel.length, 100, 100);
  int h = 0;
  for (int i = 0; i < colorWheel.length; i++) {
    colorWheel[i] = color(h, 15, 80);
    h++;
  }
  popStyle();
  return colorWheel;
}

public void initMapper() {
  hGen = new HilbertGen(width, height);
  mapper = new PixelAudioMapper(hGen);
  chan = PixelAudioMapper.ChannelNames.L;
  mapImage = createImage(width, height, RGB);
  colors = getColors();
  mapImage.loadPixels();
  mapper.plantPixels(colors, mapImage.pixels, 0, mapper.getSize());
  mapImage.updatePixels();
}

public void initAudio() {
  this.audioIn = minim.getLineIn(Minim.MONO);
  this.audioOut = minim.getLineOut(Minim.MONO, 1024, sampleRate);
  this.audioBuffer = new MultiChannelBuffer(mapSize, 1);
  this.audioSignal = new float[mapSize];
  this.rgbSignal = new int[mapSize];
  Arrays.fill(audioSignal, 0.0f);
  this.audioLength = audioSignal.length;
  this.audioBuffer.setChannel(0, audioSignal);
  streamCap = new StreamCapture();
  // path to the folder where PixelAudio examples keep their data files 
  // such as image, audio, .json, etc.
  String daPath = sketchPath("") + "../examples_data/";
  println("daPath: ", daPath);
  anthem = minim.loadFile(daPath + "youthorchestra.wav");
}  

public void draw() {
  image(mapImage, 0, 0);
  showAudioStatus();
  if (listening) {
    drawSignal();
  }
  drawInput();
}

public void rewindAudioPlayer(AudioPlayer player, boolean playAgain) {
  player.cue(0);
  if (playAgain) player.play();
}

public void showAudioStatus() {
  textSize(18);
  StringBuffer sb = new StringBuffer();
  sb.append("Listening is "+ listening +"; ");
  if (!listenLive) {
    if (anthem.isPlaying()) {
      sb.append("source is file, playing at "+ anthem.position() +"/"+ anthem.length() +"; ");
      sb.append("muting is "+ anthem.isMuted());
    }
    else {
      sb.append("source is file, paused at "+ anthem.position() +"/"+ anthem.length() +"; ");
      sb.append("muting is "+ anthem.isMuted());
    }
  }
  else {
    sb.append("source is current device "); 
  }
  text(sb.toString(), 6, 18);
}

public void keyPressed() {
  switch (key) {
  case ' ':
    toggleListening();
    break;
  case 'p':
  case 'P':
    if (!listenLive) {
      if (anthem.isPlaying())
        anthem.pause();
      else
        anthem.loop();
    }
    break;
  case 'm':
    if (anthem.isMuted()) {
      anthem.unmute();
    }
    else {
      anthem.mute();
    }
    break;
  case 't':
    toggleAudioSource();
    break;
  case 'k': // apply the hue and saturation in the colors array to mapImage 
    mapImage.loadPixels();
    applyColor(colors, mapImage.pixels, mapper.getImageToSignalLUT());
    mapImage.updatePixels();
    break;
  case 'h':
    break;
  default:
    break;
  }
}

public void showHelp() {
  println(" * Press the 'p' key to toggle between live streaming from the built-in microphone and streaming from a file.");
  println(" * Press the spacebar to record audio from the current stream into the audio buffer and write it to the screen.");
  println(" * Click on the image to play a sample. Clicking turns off recording.");
}

public void toggleListening() {
  if (listening) {
    if (listenLive) {
      audioIn.removeListener(streamCap);
    }
    else {
      anthem.removeListener(streamCap);
    }
    listening = false;
  }
  else {
    if (listenLive) {
      audioIn.addListener(streamCap);
    }
    else {
      anthem.addListener(streamCap);
    }
    listening = true;
  }
}

public void toggleAudioSource() {
  listenLive = !listenLive;
  if (listening) {
    if (listenLive) listenToDevice();
    else listenToFile();
  }
}

public void listenToDevice() {
  listening = true;
  anthem.removeListener(streamCap);
  anthem.pause();
  audioIn.addListener(streamCap);
  listenLive = true;
}

public void listenToFile() {
  listening = true;
  audioIn.removeListener(streamCap);
  anthem.addListener(streamCap);
  anthem.loop();
  listenLive = false;
}

public void mousePressed() {
  if (listening) {
    //toggleListening();
    //if (anthem.isPlaying())
    //  anthem.pause();
  }
  // get the position in the audio buffer that corresponds to the pixel location in the image
  int samplePos = mapper.lookupSample(mouseX, mouseY);
  playSample(samplePos);
}

public int playSample(int samplePos) {
  audioSampler = new Sampler(audioBuffer, sampleRate, 8);
  // ADSR
  adsr = new ADSR(maxAmplitude, attackTime, decayTime, sustainLevel, releaseTime);
  // set amplitude for the Sampler
  audioSampler.amplitude.setLastValue(0.9f);
  // set the Sampler to begin playback at samplePos, which corresponds to the
  // place the mouse was clicked
  audioSampler.begin.setLastValue(samplePos);
  int releaseDuration = (int) (releaseTime * sampleRate); // do some calculation to include the release time.
  int duration = (int) (sampleLength * sampleRate);
  if (samplePos + sampleLength >= mapSize) {
    duration = mapSize - samplePos; // make sure we don't exceed the mapSize
    // println("----->>> duraton = " + duration);
  }
  int durationPlusRelease = duration + releaseDuration;
  int end = (samplePos + durationPlusRelease >= this.mapSize) ? this.mapSize - 1
      : samplePos + durationPlusRelease;
  // println("----->>> end = " + end);
  audioSampler.end.setLastValue(end);
  // println("----->>> audioBuffer size = "+ audioBuffer.getBufferSize());
  this.instrument = new WFSamplerInstrument(audioOut, audioSampler, adsr);
  // play command takes a duration in seconds
  float dur = duration / (float) (sampleRate);
  instrument.play(dur);
  // println("----->>> duration = "+ dur);
  // return the length of the sample
  return duration;
}

public void drawInput() {
  if (listenLive) {
    for (int i = 0; i < audioIn.bufferSize() - 1; i++) {
      line(i, 50 + audioIn.left.get(i) * 50, i + 1, 50 + audioIn.left.get(i + 1) * 50);
      line(i, 150 + audioIn.right.get(i) * 50, i + 1, 150 + audioIn.right.get(i + 1) * 50);
    }
  } else {
    for (int i = 0; i < anthem.bufferSize() - 1; i++) {
      line(i, 50 + anthem.left.get(i) * 50, i + 1, 50 + anthem.left.get(i + 1) * 50);
      line(i, 150 + anthem.right.get(i) * 50, i + 1, 150 + anthem.right.get(i + 1) * 50);
    }
  }
}

public void drawSignal() {
  rgbSignal = mapper.pluckSamplesAsRGB(audioSignal, 0, mapSize); // rgbSignal is now an array of rgb gray
  mapImage.loadPixels();
  mapper.plantPixels(rgbSignal, mapImage.pixels, 0, rgbSignal.length, chan);
  mapImage.updatePixels();
}  

/**
 * Utility method for applying hue and saturation values from a source array of RGB values
 * to the brightness values in a target array of RGB values, using a lookup table to redirect indexing.
 * 
 * @param colorSource    a source array of RGB data from which to obtain hue and saturation values
 * @param graySource     an target array of RGB data from which to obtain brightness values
 * @param lut            a lookup table, must be the same size as colorSource and graySource
 * @return the graySource array of RGB values, with hue and saturation values changed
 * @throws IllegalArgumentException if array arguments are null or if they are not the same length
 */
public int[] applyColor(int[] colorSource, int[] graySource, int[] lut) {
  if (colorSource == null || graySource == null || lut == null) 
    throw new IllegalArgumentException("colorSource, graySource and lut cannot be null.");
  if (colorSource.length != graySource.length || colorSource.length != lut.length) 
    throw new IllegalArgumentException("colorSource, graySource and lut must all have the same length.");
  // initialize a reusable array for HSB color data -- this is a way to speed up the applyColor() method
  float[] hsbPixel = new float[3];
  for (int i = 0; i < graySource.length; i++) {
    graySource[i] = PixelAudioMapper.applyColor(colorSource[lut[i]], graySource[i], hsbPixel);
  }
  return graySource;
}
